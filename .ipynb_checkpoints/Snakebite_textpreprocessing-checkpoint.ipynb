{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use of twitter to analyse global discussion regarding snakebite research\n",
    "### Author: Ryan Nazareth(Manta Ray Media), Date: 29/08/2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Text Mining/Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Importing required python libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reset -f\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import seaborn as sns\n",
    "# nltk package for Natural Language Processing \n",
    "import nltk\n",
    "import tweepy\n",
    "# import spacy\n",
    "from tweepy import Stream, OAuthHandler\n",
    "from tweepy.streaming import StreamListener\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "# nltk.download() #run this first time to download all the packages in nltk\n",
    "\n",
    "#import plotly.plotly as py\n",
    "#import plotly \n",
    "#import plotly.graph_objs as go\n",
    "import datetime\n",
    "import re\n",
    "#plotly.tools.set_credentials_file(username='ryankarlos', api_key='CL8snC9zs6IImm5vHaDB')\n",
    "\n",
    "### use encoding 'latin-1' to account for encoding errors when importing into data frame "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Download all NLTK packages "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this cell once to download all the packages and corpus in NLTK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#nltk.download() #run this first time to download all the packages in nltk\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Creating an empty dataframe or uploading older csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Run this if you want to store streaming data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweet_list = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Run this if you want to analyse previous csv. In thsi case, jump staright to section 1.8 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweet_frame = pd.read_csv('snakebite29082017.csv', sep = ',', encoding = 'latin-1') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Create an account on twitter API and generate authentication keys\n",
    "\n",
    "You need to create an account on https://apps.twitter.com/ and generate the following tokens and keys:\n",
    "ACCESS_TOKEN, ACCESS_SECRET, CONSUMER_KEY, CONSUMER_SECRET \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Variables that contains the user credentials to access Twitter API\n",
    "\n",
    "ACCESS_TOKEN = '4697687863-ZEcLqfJmVlh6CQCqjWiHy90tedLSbESb5gUeUt6'\n",
    "ACCESS_SECRET = 'htY8DCAMWMjU5j3XVvSI4Q328Bd8vkQQMeNqgdW9sg4qQ'\n",
    "CONSUMER_KEY = 'amEQkb7tXUxgtFPsT6R85MtBR'\n",
    "CONSUMER_SECRET = 'lor3v8LzzeZeVrUYux6bO4ljPDLIHZFGxT3h7VgxZiDJYCzVeP'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Using Twitter search library to search for keywords  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Aug 19 15:45:55 +0000 2017@zegirish tweeted: RT @VenomInterviews: Fighting #snake #venom in rural Myanmar â€” #antivenom #healthcare #snakebite #snakes\n",
      "\n",
      "https://t.co/buxczRLSMx https://tâ€¦\n",
      "\n",
      "Fri Aug 18 15:20:26 +0000 2017@CallMeNikkiMc tweeted: In honor of all the visitiors for the solar eclipse, the ER has stocked up on snakebite anti-venom. Welcome to #Nebraska, kids.\n",
      "\n",
      "Tue Aug 15 19:42:52 +0000 2017@delanidalton tweeted: but injecting venom into a snakebite doesn't solve the damage\n",
      "\n",
      "Tue Aug 15 18:39:02 +0000 2017@Omnicell tweeted: Preparing for the eclipse next week, #hospitals stock up on anti-venom, supplies to ready for rush:â€¦ https://t.co/cWPQrXe0t7\n",
      "\n",
      "Tue Aug 15 02:46:39 +0000 2017@New_Level_Music tweeted: RT @PunchFullPro: @New_Level_Music @UTA_united_UA Hiss, Poison and Venom ðŸ #UACrew #SnakeBite\n",
      "\n",
      "Tue Aug 15 02:40:04 +0000 2017@DataFileTech tweeted: Hospitals stocking up on snakebite venom for the #eclipse. How are you prepping? https://t.co/BueJAnkmGS\n",
      "\n",
      "Tue Aug 15 01:16:57 +0000 2017@PunchFullPro tweeted: @New_Level_Music @UTA_united_UA Hiss, Poison and Venom ðŸ #UACrew #SnakeBite\n",
      "\n",
      "Tue Aug 15 00:29:52 +0000 2017@owenkeithpaiva tweeted: RT @RAPacificBeat: Cheaper anti-venom for snakebite victims in Papua New Guinea is being developed, but it won't be ready for a while httpsâ€¦\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#keywords = ['snakebite']\n",
    "#['venomous', 'snake']\n",
    "# ['snake', 'anti-venom']\n",
    "# ['snakebite', 'venom']\n",
    "# ['envenoming']\n",
    "\n",
    "from TwitterSearch import *\n",
    "\n",
    "try:\n",
    "    tso = TwitterSearchOrder()  # create a TwitterSearchOrder object\n",
    "    tso.set_keywords(['snakebite', 'venom'] )  # let's define all words we would like to have a look for\n",
    "    #tso.set_keywords(['snake', 'anti-venom'])  # let's define all words we would like to have a look for\n",
    "    tso.set_language('en')  # we want to see English tweets only\n",
    "    tso.set_include_entities(False)  # and don't give us all those entity information\n",
    "    #tso.set_until(datetime.date(2017, 8, 12))\n",
    "    # it's about time to create a TwitterSearch object with our secret tokens\n",
    "    ts = TwitterSearch(\n",
    "        consumer_key=CONSUMER_KEY,\n",
    "        consumer_secret=CONSUMER_SECRET,\n",
    "        access_token=ACCESS_TOKEN,\n",
    "        access_token_secret=ACCESS_SECRET)\n",
    "    count = 0\n",
    "    for tweet in ts.search_tweets_iterable(tso):\n",
    "        print('%s@%s tweeted: %s' % (tweet['created_at'], tweet['user']['screen_name'], tweet['text']))\n",
    "        print()\n",
    "        count = count + 1\n",
    "        if count > 11:\n",
    "            break      ### stops printing out after 12 tweets are printed\n",
    "        tweet_list.append([tweet['id'], tweet['created_at'], tweet['user']['screen_name'], tweet['favorite_count'],\n",
    "                           tweet['retweet_count'], tweet['user']['followers_count'], tweet['user']['friends_count'],\n",
    "                           tweet['text'], tweet['user']['time_zone'], tweet['user']['location']])\n",
    "\n",
    "except TwitterSearchException as e:  # take care of all those ugly errors if there are some\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Prints all the keys in the json file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['created_at', 'source', 'retweeted_status', 'favorite_count', 'in_reply_to_screen_name', 'coordinates', 'is_quote_status', 'geo', 'metadata', 'retweet_count', 'in_reply_to_user_id', 'truncated', 'in_reply_to_status_id_str', 'retweeted', 'id', 'contributors', 'in_reply_to_user_id_str', 'favorited', 'id_str', 'in_reply_to_status_id', 'place', 'lang', 'text', 'user'])\n"
     ]
    }
   ],
   "source": [
    "print(tweet.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Storing the different tweet attributes in a dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_id</th>\n",
       "      <th>Date</th>\n",
       "      <th>User</th>\n",
       "      <th>Favourite_count</th>\n",
       "      <th>Retweet_count</th>\n",
       "      <th>Followers_count</th>\n",
       "      <th>Friends_count</th>\n",
       "      <th>Text</th>\n",
       "      <th>Time_zone</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>898934059350360068</td>\n",
       "      <td>Sat Aug 19 15:45:55 +0000 2017</td>\n",
       "      <td>zegirish</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>282</td>\n",
       "      <td>805</td>\n",
       "      <td>RT @VenomInterviews: Fighting #snake #venom in...</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "      <td>Thiruvananthapuram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>898565257580478464</td>\n",
       "      <td>Fri Aug 18 15:20:26 +0000 2017</td>\n",
       "      <td>CallMeNikkiMc</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>139</td>\n",
       "      <td>In honor of all the visitiors for the solar ec...</td>\n",
       "      <td>Mountain Time (US &amp; Canada)</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>897544135074119680</td>\n",
       "      <td>Tue Aug 15 19:42:52 +0000 2017</td>\n",
       "      <td>delanidalton</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>540</td>\n",
       "      <td>296</td>\n",
       "      <td>but injecting venom into a snakebite doesn't s...</td>\n",
       "      <td>None</td>\n",
       "      <td>Az</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>897528072953487360</td>\n",
       "      <td>Tue Aug 15 18:39:02 +0000 2017</td>\n",
       "      <td>Omnicell</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2216</td>\n",
       "      <td>2065</td>\n",
       "      <td>Preparing for the eclipse next week, #hospital...</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "      <td>Mountain View, CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>897288396397314048</td>\n",
       "      <td>Tue Aug 15 02:46:39 +0000 2017</td>\n",
       "      <td>New_Level_Music</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4752</td>\n",
       "      <td>928</td>\n",
       "      <td>RT @PunchFullPro: @New_Level_Music @UTA_united...</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "      <td>Atlanta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>897286742822322176</td>\n",
       "      <td>Tue Aug 15 02:40:04 +0000 2017</td>\n",
       "      <td>DataFileTech</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>649</td>\n",
       "      <td>1040</td>\n",
       "      <td>Hospitals stocking up on snakebite venom for t...</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "      <td>Kansas City, Missouri</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>897265824163143681</td>\n",
       "      <td>Tue Aug 15 01:16:57 +0000 2017</td>\n",
       "      <td>PunchFullPro</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>93</td>\n",
       "      <td>@New_Level_Music @UTA_united_UA Hiss, Poison a...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>897253975166025728</td>\n",
       "      <td>Tue Aug 15 00:29:52 +0000 2017</td>\n",
       "      <td>owenkeithpaiva</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>36</td>\n",
       "      <td>144</td>\n",
       "      <td>RT @RAPacificBeat: Cheaper anti-venom for snak...</td>\n",
       "      <td>Port Moresby</td>\n",
       "      <td>Port Moresby, Papua New Guinea</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              User_id                            Date             User  \\\n",
       "0  898934059350360068  Sat Aug 19 15:45:55 +0000 2017         zegirish   \n",
       "1  898565257580478464  Fri Aug 18 15:20:26 +0000 2017    CallMeNikkiMc   \n",
       "2  897544135074119680  Tue Aug 15 19:42:52 +0000 2017     delanidalton   \n",
       "3  897528072953487360  Tue Aug 15 18:39:02 +0000 2017         Omnicell   \n",
       "4  897288396397314048  Tue Aug 15 02:46:39 +0000 2017  New_Level_Music   \n",
       "5  897286742822322176  Tue Aug 15 02:40:04 +0000 2017     DataFileTech   \n",
       "6  897265824163143681  Tue Aug 15 01:16:57 +0000 2017     PunchFullPro   \n",
       "7  897253975166025728  Tue Aug 15 00:29:52 +0000 2017   owenkeithpaiva   \n",
       "\n",
       "   Favourite_count  Retweet_count  Followers_count  Friends_count  \\\n",
       "0                0              4              282            805   \n",
       "1                2              0               52            139   \n",
       "2                3              0              540            296   \n",
       "3                0              0             2216           2065   \n",
       "4                0              1             4752            928   \n",
       "5                0              0              649           1040   \n",
       "6                4              1              130             93   \n",
       "7                0              2               36            144   \n",
       "\n",
       "                                                Text  \\\n",
       "0  RT @VenomInterviews: Fighting #snake #venom in...   \n",
       "1  In honor of all the visitiors for the solar ec...   \n",
       "2  but injecting venom into a snakebite doesn't s...   \n",
       "3  Preparing for the eclipse next week, #hospital...   \n",
       "4  RT @PunchFullPro: @New_Level_Music @UTA_united...   \n",
       "5  Hospitals stocking up on snakebite venom for t...   \n",
       "6  @New_Level_Music @UTA_united_UA Hiss, Poison a...   \n",
       "7  RT @RAPacificBeat: Cheaper anti-venom for snak...   \n",
       "\n",
       "                     Time_zone                        Location  \n",
       "0   Pacific Time (US & Canada)              Thiruvananthapuram  \n",
       "1  Mountain Time (US & Canada)                                  \n",
       "2                         None                             Az   \n",
       "3   Eastern Time (US & Canada)               Mountain View, CA  \n",
       "4   Eastern Time (US & Canada)                         Atlanta  \n",
       "5   Central Time (US & Canada)           Kansas City, Missouri  \n",
       "6                         None                                  \n",
       "7                 Port Moresby  Port Moresby, Papua New Guinea  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_frame = pd.DataFrame(tweet_list, columns=(\n",
    "    'User_id', 'Date', 'User', 'Favourite_count', 'Retweet_count', 'Followers_count', 'Friends_count', 'Text',\n",
    "    'Time_zone',\n",
    "    'Location'))\n",
    "tweet_frame.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8 Dropping tweets with null values and creating separate column for cleaned tweet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### Dropping tweets with null values #################\n",
    "tweet_frame = tweet_frame.dropna(subset=['Text'])\n",
    "tweet_frame = tweet_frame.reset_index(drop = True)\n",
    "\n",
    "#### keep hold of original tweets and create a seperate column \n",
    "# for doing preprocessing and creating cleaned tweets\n",
    "tweet_frame['cleaned'] =  tweet_frame['Text'].values\n",
    "\n",
    "#### removing tweets with certain irrelevant users posting and receiving tweets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.9 Filtering users who post unrelated 'snakebite' topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are topics unrelated to snakebite research but also use the word 'snakebite' sucn as beer (snakebite), darts player (snakebite), troll words related to snake/snakebite, defence organsiation etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filter_users = ['SnakeBite_Fc', 'lady_snakebite', 'Lady Snakebite','Spartan', 'snakebitewright', 'JoRoNoMo', 'snakebite_1974', \n",
    "                'WalkaboutBourno', 'PhilTaylor','Wookiewobbles', 'KnowYourVideo', 'LaGucciVida', 'jadinearnold','injytech', \n",
    "                'SarahFilmBooth', 'Ventieus', 'ChopperNews', 'Thebyrdgrl', 'livedarts', 'BetVictor', 'HonestlyWhite', '_succ_'\n",
    "                'Betfred', 'videogameurl', 'snakebite', 'amandaorson' , 'snakebitekorf', 'snakebite_part2', 'One_Step_Events',\n",
    "                'Puntfair', 'snakebite_part2', 'snakebite_2tite', 'snakebite', 'defencepk', 'BostonRockRadio', 'Snakebite_X',\n",
    "                'MLGACE', 'G1g_junkie']\n",
    "\n",
    "def removeusers(tweet_frame):\n",
    "    row = list()\n",
    "    for i in range(0, tweet_frame.shape[0]):\n",
    "        if any (word in tweet_frame['User'][i] for word in filter_users):\n",
    "            row.append(i)\n",
    "        elif filter((lambda x: re.search('game', x)),tweet_frame['User'][i]) == ('game'):\n",
    "            row.append(i)\n",
    "        elif any(word in tweet_frame['Text'][i] for word in filter_users):\n",
    "            row.append(i) \n",
    "    \n",
    "    tweet_frame.drop(tweet_frame.index[row], inplace=True)\n",
    "    return tweet_frame\n",
    "           \n",
    "removeusers(tweet_frame)\n",
    "tweet_frame = tweet_frame.reset_index(drop = True)   ### resets the index when removing rows and does not add a  new column index      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.10 Producing a data cleaning pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This involes the following procedures: Removing stopwords, punctuation, @, http links, tokensiation, lemmitisation/stemming. The words in the cleaned tweet is then stored as an array in the 'cleaned' column. \n",
    "\n",
    "Stop words are the words we want to filter out before training the classifier. These are usually high frequency words that arenâ€™t giving any additional information to our labeling. In fact, they actually confuse our classifier. In English, they could be the, is, at, which, and on. \n",
    "Lemmatisation or stemming is the process for reducing inflected words to their word stem (base form). e.g. infected and infectious become infect. The classfier does not understand that infected and infectious come from the same root word and treat them as different. By stemming them, it groups the frequencies of different inflection to just one term.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn']\n"
     ]
    }
   ],
   "source": [
    "                \n",
    "print(stopwords.words('english'))\n",
    "stop = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself',\n",
    "             'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself',\n",
    "             'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these',\n",
    "             'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do',\n",
    "             'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while',\n",
    "             'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before',\n",
    "             'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again',\n",
    "             'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each',\n",
    "             'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than',\n",
    "             'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've',\n",
    "             'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn',\n",
    "             'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn']\n",
    "\n",
    "import re\n",
    "import string\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer, SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "lemmatiser = WordNetLemmatizer()\n",
    "\n",
    "cleaned_tweet = []\n",
    "def cleantweet(tweet_frame, cleaned_tweet):\n",
    "    \n",
    "    for data in tweet_frame['cleaned']:\n",
    "        str_no_hyperlinks=re.sub(r'https?:\\/\\/.*\\/\\w*','',data)  ## removes links\n",
    "        str_lower =str_no_hyperlinks.lower()  ## sets the resulting tweet to lowercase\n",
    "        str_no_username = re.sub(r'(?:@[\\w_]+)', '', str_lower) # removes @-mentions \n",
    "        str_no_username = str_no_username.strip() ## splits the tweet into words\n",
    "        exclude = set(string.punctuation) ## setting a list of punctuations\n",
    "        str_no_punc = \"\".join(word for word in str_no_username if word not in exclude) ## removes punctuation\n",
    "        tokenised = nltk.word_tokenize(str_no_punc) ##tokenisation \n",
    "        stop_free = [word for word in tokenised if word not in stopwords.words('english')] ## remove stop words\n",
    "        word_lemm = [lemmatiser.lemmatize(t) for t in stop_free] ## lemmatisation\n",
    "        #word_stem = [stemmer.stem(i) for i in word_lemm]  ## optional stemming\n",
    "        cleaned_tweet.append(word_lemm)  ##append the resulting vector to the cleaned tweet column\n",
    "                  \n",
    "    return cleaned_tweet\n",
    "\n",
    "cleantweet(tweet_frame, cleaned_tweet)\n",
    "\n",
    "tweet_frame['cleaned'] = cleaned_tweet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.11 Filtering out irrelevant tweets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tweets are filtered out by one of two methods: \n",
    "*If keywords in the cleaned vector are present in the keywords_filter list - then tweet is removed.\n",
    "OR \n",
    "*If length of the word vector is less than 6, then the tweet is removed. This value was found to be optimal after tweaking. Irrelevant tweets (especially people who troll) are more likely to be short in length with fewer words (after stopword removal)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_id</th>\n",
       "      <th>Date</th>\n",
       "      <th>User</th>\n",
       "      <th>Favourite_count</th>\n",
       "      <th>Retweet_count</th>\n",
       "      <th>Followers_count</th>\n",
       "      <th>Friends_count</th>\n",
       "      <th>Text</th>\n",
       "      <th>Time_zone</th>\n",
       "      <th>Location</th>\n",
       "      <th>cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>897528072953487360</td>\n",
       "      <td>Tue Aug 15 18:39:02 +0000 2017</td>\n",
       "      <td>Omnicell</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2216</td>\n",
       "      <td>2065</td>\n",
       "      <td>Preparing for the eclipse next week, #hospital...</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "      <td>Mountain View, CA</td>\n",
       "      <td>[preparing, eclipse, next, week, hospital, sto...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              User_id                            Date      User  \\\n",
       "0  897528072953487360  Tue Aug 15 18:39:02 +0000 2017  Omnicell   \n",
       "\n",
       "   Favourite_count  Retweet_count  Followers_count  Friends_count  \\\n",
       "0                0              0             2216           2065   \n",
       "\n",
       "                                                Text  \\\n",
       "0  Preparing for the eclipse next week, #hospital...   \n",
       "\n",
       "                    Time_zone           Location  \\\n",
       "0  Eastern Time (US & Canada)  Mountain View, CA   \n",
       "\n",
       "                                             cleaned  \n",
       "0  [preparing, eclipse, next, week, hospital, sto...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "keywords_filter  =['wright', 'dart', 'race', 'bike', 'cmon', 'cmmon', 'piercings', 'piercing','pierced','fuck', 'fuckin', 'tit','fucking',\n",
    "                   'music', 'radio','cunt', 'dick','ass', 'currant', 'guinness', 'hockey', 'pee', 'dj','drink', 'peter', 'phil', \n",
    "                   'taylor', 'horror', 'movie', 'film', 'beer', 'whiskey','pint', 'text','vodka', 'apple', 'stream', \n",
    "                   'streaming', 'booze', 'knight', 'game','code', 'blackcurrant', 'blackcurrent', 'cider', 'match', \n",
    "                   'ring', 'piercing', 'tattoo','rt', 'rts','@', 'drinking', 'drinkin', 'retweet','drink', 'drunk', 'uni', 'player',\n",
    "                   'swig', 'served', 'serve', 'serves', 'bar', 'worldmatchplay', 'worldmatchplaydarts', 'happy', 'love',\n",
    "                   'religion', 'gnight', 'sneaky', 'sic', 'lovely', 'played', 'plays', 'play', 'played', 'sunday', 'alcohol','saturday',\n",
    "                   'gin','brew', 'god', 'pub', 'lol', 'lmao','lmfao','tix', 'dm','tickets', 'deal', 'boozer', 'nowplaying', 'night',\n",
    "                   'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'war', 'party', 'cold', 'oh', 'ohh', 'ooohhh', 'oohh', 'ha', 'haha', 'hah',\n",
    "                   'nightlife', 'partying', 'raspberry', 'grill', 'bill', 'uacrew', 'connector', 'connecting', 'wire', 'sell', 'selling', 'purple',\n",
    "                   'gun', 'violence', 'rating', 'movie', 'video', 'weight', 'nip', 'turtle', 'chronicles', 'chronicle', 'bus', 'dollar', 'coin',\n",
    "                   'chopper', 'bust', 'busted', 'pistol', 'boom', 'glove', 'diy', 'enter', 'evil', 'draught', 'cook', 'cooking', 'vid', 'diesel',\n",
    "                    'jungle', 'juice', 'stud', 'ring', 'lip', 'nipple', 'staff', 'dodgy', 'win' , 'loss', 'fan', 'band', 'semifinal', 'melbournedarts',\n",
    "                    'winner', 'loser', 'compete','dinner', 'restaurant', 'mvp', 'goat','fc', 'sing', 'album', 'sung', 'chart', 'classic', 'sport', 'dc',\n",
    "                    'omg', 'quid', 'trump', 'nazi','driver', 'Christ', 'love', 'silverware', 'caline', 'grimlock', 'mayan', 'necklace', 'unibet180', 'beat',\n",
    "                    'bos', 'boss', 'wade', 'mary']\n",
    "\n",
    "def filtertweets(tweet_frame):\n",
    "    row = list()\n",
    "    for i in range(0, tweet_frame.shape[0]):\n",
    "        if any (word in tweet_frame['cleaned'][i] for word in keywords_filter):\n",
    "            row.append(i)\n",
    "        if len(tweet_frame['cleaned'][i]) <6:\n",
    "            row.append(i)\n",
    "    tweet_frame.drop(tweet_frame.index[row], inplace=True)\n",
    "    return tweet_frame\n",
    "\n",
    "filtertweets(tweet_frame)\n",
    "tweet_frame.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.12 Concatenating results to existing csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweet_frame.to_csv('snakebite29082017.csv', header = False, mode = 'a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.13 Viewing the entire csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>User_id</th>\n",
       "      <th>Date</th>\n",
       "      <th>User</th>\n",
       "      <th>Favourite_count</th>\n",
       "      <th>Retweet_count</th>\n",
       "      <th>Followers_count</th>\n",
       "      <th>Friends_count</th>\n",
       "      <th>Text</th>\n",
       "      <th>Time_zone</th>\n",
       "      <th>Location</th>\n",
       "      <th>cleaned</th>\n",
       "      <th>Timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jul-03</td>\n",
       "      <td>zone_get</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Do you know how to avoid and treat a venomous ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['know', 'avoid', 'treat', 'venomous', 'snake'...</td>\n",
       "      <td>18:51:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jul-03</td>\n",
       "      <td>dougcarl88181</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Snakebite could have killed toddler if not for...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Phoenix, AZ</td>\n",
       "      <td>['snakebite', 'could', 'killed', 'toddler', 'q...</td>\n",
       "      <td>16:02:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jul-04</td>\n",
       "      <td>MadisonReptiles</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Check out this article on an educational infog...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Madison, WI</td>\n",
       "      <td>['check', 'article', 'educational', 'infograph...</td>\n",
       "      <td>21:55:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>99</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jul-04</td>\n",
       "      <td>zone_get</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Do you know how to avoid and treat a venomous ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['know', 'avoid', 'treat', 'venomous', 'snake'...</td>\n",
       "      <td>17:10:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>125</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jul-05</td>\n",
       "      <td>Mafaranga</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Snakebite patients with evidence of envenoming...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Kampala, Uganda</td>\n",
       "      <td>['snakebite', 'patient', 'evidence', 'envenomi...</td>\n",
       "      <td>13:56:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>124</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jul-05</td>\n",
       "      <td>PambeleInvest</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Thank you African Snakebite Institute for this...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Richards Bay, South Africa</td>\n",
       "      <td>['thank', 'african', 'snakebite', 'institute',...</td>\n",
       "      <td>13:47:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>87</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jul-06</td>\n",
       "      <td>andanin</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Snakebite is an urban though it is perceived a...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Geelong, Victoria</td>\n",
       "      <td>['snakebite', 'urban', 'though', 'perceived', ...</td>\n",
       "      <td>00:27:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>86</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jul-06</td>\n",
       "      <td>andanin</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Snakebite was classed as a 'violent death', as...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Geelong, Victoria</td>\n",
       "      <td>['snakebite', 'classed', 'violent', 'death', '...</td>\n",
       "      <td>00:05:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>61</td>\n",
       "      <td>8.940000e+17</td>\n",
       "      <td>Aug-06</td>\n",
       "      <td>559houses</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>81120.0</td>\n",
       "      <td>62643.0</td>\n",
       "      <td>Dog 1, Rattlesnake 0: San Juan Capistrano Pet ...</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "      <td>Fresno California</td>\n",
       "      <td>['dog', '1', 'rattlesnake', '0', 'san', 'juan'...</td>\n",
       "      <td>18:57:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>14</td>\n",
       "      <td>8.950000e+17</td>\n",
       "      <td>Aug-07</td>\n",
       "      <td>holed001</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>354.0</td>\n",
       "      <td>623.0</td>\n",
       "      <td>@Single_CLE @suburb_single No bouncers these d...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Rochester, Kent</td>\n",
       "      <td>['bouncer', 'day', 'strict', 'lad', '16', 'coc...</td>\n",
       "      <td>15:06:09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0       User_id    Date             User  Favourite_count  \\\n",
       "0          20           NaN  Jul-03         zone_get                1   \n",
       "1          19           NaN  Jul-03    dougcarl88181                0   \n",
       "2         100           NaN  Jul-04  MadisonReptiles                1   \n",
       "3          99           NaN  Jul-04         zone_get                0   \n",
       "4         125           NaN  Jul-05        Mafaranga                0   \n",
       "5         124           NaN  Jul-05    PambeleInvest                1   \n",
       "6          87           NaN  Jul-06          andanin                1   \n",
       "7          86           NaN  Jul-06          andanin                1   \n",
       "8          61  8.940000e+17  Aug-06        559houses                3   \n",
       "9          14  8.950000e+17  Aug-07         holed001                0   \n",
       "\n",
       "   Retweet_count  Followers_count  Friends_count  \\\n",
       "0              0              NaN            NaN   \n",
       "1              0              NaN            NaN   \n",
       "2              0              NaN            NaN   \n",
       "3              0              NaN            NaN   \n",
       "4              0              NaN            NaN   \n",
       "5              0              NaN            NaN   \n",
       "6              0              NaN            NaN   \n",
       "7              0              NaN            NaN   \n",
       "8              0          81120.0        62643.0   \n",
       "9              0            354.0          623.0   \n",
       "\n",
       "                                                Text  \\\n",
       "0  Do you know how to avoid and treat a venomous ...   \n",
       "1  Snakebite could have killed toddler if not for...   \n",
       "2  Check out this article on an educational infog...   \n",
       "3  Do you know how to avoid and treat a venomous ...   \n",
       "4  Snakebite patients with evidence of envenoming...   \n",
       "5  Thank you African Snakebite Institute for this...   \n",
       "6  Snakebite is an urban though it is perceived a...   \n",
       "7  Snakebite was classed as a 'violent death', as...   \n",
       "8  Dog 1, Rattlesnake 0: San Juan Capistrano Pet ...   \n",
       "9  @Single_CLE @suburb_single No bouncers these d...   \n",
       "\n",
       "                    Time_zone                    Location  \\\n",
       "0                         NaN                         NaN   \n",
       "1                         NaN                 Phoenix, AZ   \n",
       "2                         NaN                 Madison, WI   \n",
       "3                         NaN                         NaN   \n",
       "4                         NaN             Kampala, Uganda   \n",
       "5                         NaN  Richards Bay, South Africa   \n",
       "6                         NaN           Geelong, Victoria   \n",
       "7                         NaN           Geelong, Victoria   \n",
       "8  Pacific Time (US & Canada)           Fresno California   \n",
       "9                         NaN             Rochester, Kent   \n",
       "\n",
       "                                             cleaned Timestamp  \n",
       "0  ['know', 'avoid', 'treat', 'venomous', 'snake'...  18:51:52  \n",
       "1  ['snakebite', 'could', 'killed', 'toddler', 'q...  16:02:26  \n",
       "2  ['check', 'article', 'educational', 'infograph...  21:55:43  \n",
       "3  ['know', 'avoid', 'treat', 'venomous', 'snake'...  17:10:00  \n",
       "4  ['snakebite', 'patient', 'evidence', 'envenomi...  13:56:31  \n",
       "5  ['thank', 'african', 'snakebite', 'institute',...  13:47:32  \n",
       "6  ['snakebite', 'urban', 'though', 'perceived', ...  00:27:59  \n",
       "7  ['snakebite', 'classed', 'violent', 'death', '...  00:05:27  \n",
       "8  ['dog', '1', 'rattlesnake', '0', 'san', 'juan'...  18:57:00  \n",
       "9  ['bouncer', 'day', 'strict', 'lad', '16', 'coc...  15:06:09  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_frame = pd.read_csv('snakebite29082017.csv', sep = ',', encoding = 'latin-1') \n",
    "tweet_frame.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tableau plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href = \"https://public.tableau.com/profile/ryan.nazareth#!/vizhome/tweetsovertime_0/Numberoftweetsovertime\"> Tableau - Tweets over time</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href = \"https://public.tableau.com/profile/ryan.nazareth#!/vizhome/Numberofretweetsandfavourites/Numberofretweetsandfavourites?publish=yes\"> Tableau Number of retweets and favs </a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href = \"https://public.tableau.com/profile/ryan.nazareth#!/vizhome/Userposts/Numberofuserposts?publish=yes\"> Tableau User posts </a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## From tableau\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML(\"\"\"<a href = \"https://public.tableau.com/profile/ryan.nazareth#!/vizhome/tweetsovertime_0/Numberoftweetsovertime\"> Tableau - Tweets over time</a>\"\"\"))\n",
    "display(HTML(\"\"\"<a href = \"https://public.tableau.com/profile/ryan.nazareth#!/vizhome/Numberofretweetsandfavourites/Numberofretweetsandfavourites?publish=yes\"> Tableau Number of retweets and favs </a>\"\"\"))\n",
    "display(HTML(\"\"\"<a href = \"https://public.tableau.com/profile/ryan.nazareth#!/vizhome/Userposts/Numberofuserposts?publish=yes\"> Tableau User posts </a>\"\"\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
