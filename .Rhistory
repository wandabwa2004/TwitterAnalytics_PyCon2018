"chartreuse3","darkorchid2", "darkorchid3", "darkorchid4",
"firebrick1", "firebrick2", "firebrick3",
"purple", "purple1", "purple2", "purple3", "purple4", "red", "red1",
"red2", "red3", "red4", "violetred1",
"violetred2", "violetred3", "violetred4")
set.seed(4363)
wordcloud(sort_two$bitoken,sort_two$Freq,random.order=FALSE,min.freq = 20, scale=c(2.2,.5), colors = sample(color, 20, replace=TRUE ))
ggplot(head(sort_two,30), aes(reorder(bitoken,Freq), Freq)) +
geom_bar(stat = "identity") + coord_flip() +
xlab("Bigrams") + ylab("Frequency") +
ggtitle("Most frequent bigrams")
keywords <- c("venom", "snake", "snakebite", "antisnake")
myCorpus <- Corpus(VectorSource(snakebite$Text))
myCorpus <- tm_map(myCorpus, tolower)
myCorpus <- tm_map(myCorpus, removePunctuation)
myCorpus <- tm_map(myCorpus, removeNumbers)
myCorpus <- tm_map(myCorpus, removeWords, stopwords("english"))
myCorpus <- tm_map(myCorpus, removeWords, keywords)
myCorpus <- tm_map(myCorpus, content_transformer(function(x) gsub(x, pattern = "\\<nm\\>", replacement = "n8m")))
myCorpus <- tm_map(myCorpus, content_transformer(function(x) gsub(x, pattern = "[^[:alnum:][:blank:]?&/\\-]", replacement = "")))
myCorpus <- tm_map(myCorpus, content_transformer(function(x) gsub(x, pattern = "u00..", replacement = "")))
myCorpus <- iconv(myCorpus, from = "UTF-8", to = "ASCII", sub = "")  ## removing non asci charatcers
token_delim <- " \\t\\r\\n.!?,;\"()"
bitoken <- NGramTokenizer(myCorpus, Weka_control(min=2,max=2, delimiters = token_delim))
two_word <- data.frame(table(bitoken))
sort_two <- two_word[order(two_word$Freq,decreasing=TRUE),]
head(sort_two, 20)
color <- c("blue3", "blue4", "blueviolet", "brown1", "brown2",
"brown3", "darkgoldenrod3", "darkgoldenrod4",
"chocolate2", "chocolate3", "chocolate1",
"green4","darkorchid2", "darkorchid3", "darkorchid4",
"firebrick1", "firebrick2", "firebrick3",
"purple", "purple1", "purple2", "purple3", "purple4", "red", "red1",
"red2", "red3", "red4", "violetred1",
"violetred2", "violetred3", "violetred4")
set.seed(4363)
wordcloud(sort_two$bitoken,sort_two$Freq,random.order=FALSE,min.freq = 20, scale=c(2.2,.5), colors = sample(color, 20, replace=TRUE ))
ggplot(head(sort_two,30), aes(reorder(bitoken,Freq), Freq)) +
geom_bar(stat = "identity") + coord_flip() +
xlab("Bigrams") + ylab("Frequency") +
ggtitle("Most frequent bigrams")
keywords <- c("venom", "snake", "snakebite", "antisnake")
myCorpus <- Corpus(VectorSource(snakebite$Text))
myCorpus <- tm_map(myCorpus, tolower)
myCorpus <- tm_map(myCorpus, removePunctuation)
myCorpus <- tm_map(myCorpus, removeNumbers)
myCorpus <- tm_map(myCorpus, removeWords, stopwords("english"))
myCorpus <- tm_map(myCorpus, removeWords, keywords)
myCorpus <- tm_map(myCorpus, content_transformer(function(x) gsub(x, pattern = "\\<nm\\>", replacement = "n8m")))
myCorpus <- tm_map(myCorpus, content_transformer(function(x) gsub(x, pattern = "[^[:alnum:][:blank:]?&/\\-]", replacement = "")))
myCorpus <- tm_map(myCorpus, content_transformer(function(x) gsub(x, pattern = "u00..", replacement = "")))
myCorpus <- iconv(myCorpus, from = "UTF-8", to = "ASCII", sub = "")  ## removing non asci charatcers
token_delim <- " \\t\\r\\n.!?,;\"()"
bitoken <- NGramTokenizer(myCorpus, Weka_control(min=2,max=2, delimiters = token_delim))
two_word <- data.frame(table(bitoken))
sort_two <- two_word[order(two_word$Freq,decreasing=TRUE),]
head(sort_two, 20)
color <- c("blue3", "blue4", "blueviolet", "brown1", "brown2",
"brown3", "darkgoldenrod3", "darkgoldenrod4",
"chocolate2", "chocolate3", "chocolate1",
"green4","darkorchid2", "darkorchid3", "darkorchid4",
"firebrick1", "firebrick2", "firebrick3",
"purple", "purple1", "purple2", "purple3", "purple4", "red", "red1",
"red2", "red3", "red4")
set.seed(4363)
wordcloud(sort_two$bitoken,sort_two$Freq,random.order=FALSE,min.freq = 20, scale=c(2.2,.5), colors = sample(color, 20, replace=TRUE ))
ggplot(head(sort_two,30), aes(reorder(bitoken,Freq), Freq)) +
geom_bar(stat = "identity") + coord_flip() +
xlab("Bigrams") + ylab("Frequency") +
ggtitle("Most frequent bigrams")
keywords <- c("venom", "snake", "snakebite", "antisnake")
myCorpus <- Corpus(VectorSource(snakebite$Text))
myCorpus <- tm_map(myCorpus, tolower)
myCorpus <- tm_map(myCorpus, removePunctuation)
myCorpus <- tm_map(myCorpus, removeNumbers)
myCorpus <- tm_map(myCorpus, removeWords, stopwords("english"))
myCorpus <- tm_map(myCorpus, removeWords, keywords)
myCorpus <- tm_map(myCorpus, content_transformer(function(x) gsub(x, pattern = "\\<nm\\>", replacement = "n8m")))
myCorpus <- tm_map(myCorpus, content_transformer(function(x) gsub(x, pattern = "[^[:alnum:][:blank:]?&/\\-]", replacement = "")))
myCorpus <- tm_map(myCorpus, content_transformer(function(x) gsub(x, pattern = "u00..", replacement = "")))
myCorpus <- iconv(myCorpus, from = "UTF-8", to = "ASCII", sub = "")  ## removing non asci charatcers
token_delim <- " \\t\\r\\n.!?,;\"()"
bitoken <- NGramTokenizer(myCorpus, Weka_control(min=2,max=2, delimiters = token_delim))
two_word <- data.frame(table(bitoken))
sort_two <- two_word[order(two_word$Freq,decreasing=TRUE),]
head(sort_two, 20)
color <- c("blue3", "blue4", "blueviolet", "brown1", "brown2",
"brown3", "darkgoldenrod3", "darkgoldenrod4",
"chocolate2", "chocolate3", "chocolate1",
"green4","darkorchid2", "darkorchid3", "darkorchid4",
"firebrick1", "firebrick2", "firebrick3",
"purple", "purple1", "purple2", "purple3", "purple4", "red", "red1",
"red2", "red3", "red4")
set.seed(4363)
wordcloud(sort_two$bitoken,sort_two$Freq,random.order=FALSE,min.freq = 20, scale=c(2.2,.5), colors = sample(color, 10, replace=TRUE ))
ggplot(head(sort_two,30), aes(reorder(bitoken,Freq), Freq)) +
geom_bar(stat = "identity") + coord_flip() +
xlab("Bigrams") + ylab("Frequency") +
ggtitle("Most frequent bigrams")
keywords <- c("venom", "snake", "snakebite", "antisnake")
myCorpus <- Corpus(VectorSource(snakebite$Text))
myCorpus <- tm_map(myCorpus, tolower)
myCorpus <- tm_map(myCorpus, removePunctuation)
myCorpus <- tm_map(myCorpus, removeNumbers)
myCorpus <- tm_map(myCorpus, removeWords, stopwords("english"))
myCorpus <- tm_map(myCorpus, removeWords, keywords)
myCorpus <- tm_map(myCorpus, content_transformer(function(x) gsub(x, pattern = "\\<nm\\>", replacement = "n8m")))
myCorpus <- tm_map(myCorpus, content_transformer(function(x) gsub(x, pattern = "[^[:alnum:][:blank:]?&/\\-]", replacement = "")))
myCorpus <- tm_map(myCorpus, content_transformer(function(x) gsub(x, pattern = "u00..", replacement = "")))
myCorpus <- iconv(myCorpus, from = "UTF-8", to = "ASCII", sub = "")  ## removing non asci charatcers
token_delim <- " \\t\\r\\n.!?,;\"()"
bitoken <- NGramTokenizer(myCorpus, Weka_control(min=2,max=2, delimiters = token_delim))
two_word <- data.frame(table(bitoken))
sort_two <- two_word[order(two_word$Freq,decreasing=TRUE),]
head(sort_two, 20)
color <- c("blue3", "blue4", "darkgoldenrod3", "darkgoldenrod4",
"chocolate1", "chocolate3", "chocolate1",
"green4","darkorchid3", "darkorchid4",
"firebrick1", "firebrick2", "firebrick3",
"purple3", "purple4")
set.seed(4363)
wordcloud(sort_two$bitoken,sort_two$Freq,random.order=FALSE,min.freq = 20, scale=c(2.2,.5), colors = sample(color, 15, replace=TRUE ))
ggplot(head(sort_two,30), aes(reorder(bitoken,Freq), Freq)) +
geom_bar(stat = "identity") + coord_flip() +
xlab("Bigrams") + ylab("Frequency") +
ggtitle("Most frequent bigrams")
keywords <- c("venom", "snake", "snakebite", "antisnake")
myCorpus <- Corpus(VectorSource(snakebite$Text))
myCorpus <- tm_map(myCorpus, tolower)
myCorpus <- tm_map(myCorpus, removePunctuation)
myCorpus <- tm_map(myCorpus, removeNumbers)
myCorpus <- tm_map(myCorpus, removeWords, stopwords("english"))
myCorpus <- tm_map(myCorpus, removeWords, keywords)
myCorpus <- tm_map(myCorpus, content_transformer(function(x) gsub(x, pattern = "\\<nm\\>", replacement = "n8m")))
myCorpus <- tm_map(myCorpus, content_transformer(function(x) gsub(x, pattern = "[^[:alnum:][:blank:]?&/\\-]", replacement = "")))
myCorpus <- tm_map(myCorpus, content_transformer(function(x) gsub(x, pattern = "u00..", replacement = "")))
myCorpus <- iconv(myCorpus, from = "UTF-8", to = "ASCII", sub = "")  ## removing non asci charatcers
token_delim <- " \\t\\r\\n.!?,;\"()"
bitoken <- NGramTokenizer(myCorpus, Weka_control(min=2,max=2, delimiters = token_delim))
two_word <- data.frame(table(bitoken))
sort_two <- two_word[order(two_word$Freq,decreasing=TRUE),]
head(sort_two, 20)
color <- c("blue3", "blue4", "darkgoldenrod3", "darkgoldenrod4",
"chocolate1", "chocolate3", "chocolate1",
"green4","darkorchid3", "darkorchid4",
"firebrick1", "firebrick2", "firebrick3",
"purple3")
set.seed(4363)
wordcloud(sort_two$bitoken,sort_two$Freq,random.order=FALSE,min.freq = 20, scale=c(2.2,.5), colors = sample(color, 15, replace=TRUE ))
ggplot(head(sort_two,30), aes(reorder(bitoken,Freq), Freq)) +
geom_bar(stat = "identity") + coord_flip() +
xlab("Bigrams") + ylab("Frequency") +
ggtitle("Most frequent bigrams")
keywords <- c("venom", "snake", "snakebite", "antisnake")
myCorpus <- Corpus(VectorSource(snakebite$Text))
myCorpus <- tm_map(myCorpus, tolower)
myCorpus <- tm_map(myCorpus, removePunctuation)
myCorpus <- tm_map(myCorpus, removeNumbers)
myCorpus <- tm_map(myCorpus, removeWords, stopwords("english"))
myCorpus <- tm_map(myCorpus, removeWords, keywords)
myCorpus <- tm_map(myCorpus, content_transformer(function(x) gsub(x, pattern = "\\<nm\\>", replacement = "n8m")))
myCorpus <- tm_map(myCorpus, content_transformer(function(x) gsub(x, pattern = "[^[:alnum:][:blank:]?&/\\-]", replacement = "")))
myCorpus <- tm_map(myCorpus, content_transformer(function(x) gsub(x, pattern = "u00..", replacement = "")))
myCorpus <- iconv(myCorpus, from = "UTF-8", to = "ASCII", sub = "")  ## removing non asci charatcers
token_delim <- " \\t\\r\\n.!?,;\"()"
bitoken <- NGramTokenizer(myCorpus, Weka_control(min=2,max=2, delimiters = token_delim))
two_word <- data.frame(table(bitoken))
sort_two <- two_word[order(two_word$Freq,decreasing=TRUE),]
head(sort_two, 20)
color <- c("blue3", "blue4", "darkgoldenrod3", "darkgoldenrod4",
"chocolate1", "chocolate3", "chocolate1",
"green4","darkorchid3", "darkorchid4",
"firebrick1", "firebrick2", "firebrick3",
"purple3")
set.seed(4363)
wordcloud(sort_two$bitoken,sort_two$Freq,random.order=FALSE,min.freq = 20, scale=c(1.9,.5), colors = sample(color, 15, replace=TRUE ))
ggplot(head(sort_two,30), aes(reorder(bitoken,Freq), Freq)) +
geom_bar(stat = "identity") + coord_flip() +
xlab("Bigrams") + ylab("Frequency") +
ggtitle("Most frequent bigrams")
keywords <- c("venom", "snake", "snakebite", "antisnake")
myCorpus <- Corpus(VectorSource(snakebite$Text))
myCorpus <- tm_map(myCorpus, tolower)
myCorpus <- tm_map(myCorpus, removePunctuation)
myCorpus <- tm_map(myCorpus, removeNumbers)
myCorpus <- tm_map(myCorpus, removeWords, stopwords("english"))
myCorpus <- tm_map(myCorpus, removeWords, keywords)
myCorpus <- tm_map(myCorpus, content_transformer(function(x) gsub(x, pattern = "\\<nm\\>", replacement = "n8m")))
myCorpus <- tm_map(myCorpus, content_transformer(function(x) gsub(x, pattern = "[^[:alnum:][:blank:]?&/\\-]", replacement = "")))
myCorpus <- tm_map(myCorpus, content_transformer(function(x) gsub(x, pattern = "u00..", replacement = "")))
myCorpus <- iconv(myCorpus, from = "UTF-8", to = "ASCII", sub = "")  ## removing non asci charatcers
token_delim <- " \\t\\r\\n.!?,;\"()"
bitoken <- NGramTokenizer(myCorpus, Weka_control(min=2,max=2, delimiters = token_delim))
two_word <- data.frame(table(bitoken))
sort_two <- two_word[order(two_word$Freq,decreasing=TRUE),]
head(sort_two, 20)
color <- c("blue3", "blue4", "darkgoldenrod3", "darkgoldenrod4",
"chocolate1", "chocolate3", "chocolate1",
"green4","darkorchid3", "darkorchid4",
"firebrick1", "firebrick2", "firebrick3",
"purple3")
set.seed(4363)
wordcloud(sort_two$bitoken,sort_two$Freq,random.order=FALSE,min.freq = 20, scale=c(3,1), colors = sample(color, 15, replace=TRUE ))
ggplot(head(sort_two,30), aes(reorder(bitoken,Freq), Freq)) +
geom_bar(stat = "identity") + coord_flip() +
xlab("Bigrams") + ylab("Frequency") +
ggtitle("Most frequent bigrams")
keywords <- c("venom", "snake", "snakebite", "antisnake")
myCorpus <- Corpus(VectorSource(snakebite$Text))
myCorpus <- tm_map(myCorpus, tolower)
myCorpus <- tm_map(myCorpus, removePunctuation)
myCorpus <- tm_map(myCorpus, removeNumbers)
myCorpus <- tm_map(myCorpus, removeWords, stopwords("english"))
myCorpus <- tm_map(myCorpus, removeWords, keywords)
myCorpus <- tm_map(myCorpus, content_transformer(function(x) gsub(x, pattern = "\\<nm\\>", replacement = "n8m")))
myCorpus <- tm_map(myCorpus, content_transformer(function(x) gsub(x, pattern = "[^[:alnum:][:blank:]?&/\\-]", replacement = "")))
myCorpus <- tm_map(myCorpus, content_transformer(function(x) gsub(x, pattern = "u00..", replacement = "")))
myCorpus <- iconv(myCorpus, from = "UTF-8", to = "ASCII", sub = "")  ## removing non asci charatcers
token_delim <- " \\t\\r\\n.!?,;\"()"
bitoken <- NGramTokenizer(myCorpus, Weka_control(min=2,max=2, delimiters = token_delim))
two_word <- data.frame(table(bitoken))
sort_two <- two_word[order(two_word$Freq,decreasing=TRUE),]
head(sort_two, 20)
color <- c("blue3", "blue4", "darkgoldenrod3", "darkgoldenrod4",
"chocolate1", "chocolate3", "chocolate1",
"green4","darkorchid3", "darkorchid4",
"firebrick1", "firebrick2", "firebrick3",
"purple3")
set.seed(4363)
wordcloud(sort_two$bitoken,sort_two$Freq,random.order=FALSE,min.freq = 20, scale=c(3.5,1), colors = sample(color, 15, replace=TRUE ))
ggplot(head(sort_two,30), aes(reorder(bitoken,Freq), Freq)) +
geom_bar(stat = "identity") + coord_flip() +
xlab("Bigrams") + ylab("Frequency") +
ggtitle("Most frequent bigrams")
keywords <- c("venom", "snake", "snakebite", "antisnake")
myCorpus <- Corpus(VectorSource(snakebite$Text))
myCorpus <- tm_map(myCorpus, tolower)
myCorpus <- tm_map(myCorpus, removePunctuation)
myCorpus <- tm_map(myCorpus, removeNumbers)
myCorpus <- tm_map(myCorpus, removeWords, stopwords("english"))
myCorpus <- tm_map(myCorpus, removeWords, keywords)
myCorpus <- tm_map(myCorpus, content_transformer(function(x) gsub(x, pattern = "\\<nm\\>", replacement = "n8m")))
myCorpus <- tm_map(myCorpus, content_transformer(function(x) gsub(x, pattern = "[^[:alnum:][:blank:]?&/\\-]", replacement = "")))
myCorpus <- tm_map(myCorpus, content_transformer(function(x) gsub(x, pattern = "u00..", replacement = "")))
myCorpus <- iconv(myCorpus, from = "UTF-8", to = "ASCII", sub = "")  ## removing non asci charatcers
token_delim <- " \\t\\r\\n.!?,;\"()"
bitoken <- NGramTokenizer(myCorpus, Weka_control(min=2,max=2, delimiters = token_delim))
two_word <- data.frame(table(bitoken))
sort_two <- two_word[order(two_word$Freq,decreasing=TRUE),]
head(sort_two, 20)
color <- c("blue3", "blue4", "darkgoldenrod3", "darkgoldenrod4",
"chocolate1", "chocolate3", "chocolate1",
"green4","darkorchid3", "darkorchid4",
"firebrick1", "firebrick2", "firebrick3",
"purple3")
set.seed(4363)
wordcloud(sort_two$bitoken,sort_two$Freq,random.order=FALSE,min.freq = 20, scale=c(2,1), colors = sample(color, 15, replace=TRUE ))
ggplot(head(sort_two,30), aes(reorder(bitoken,Freq), Freq)) +
geom_bar(stat = "identity") + coord_flip() +
xlab("Bigrams") + ylab("Frequency") +
ggtitle("Most frequent bigrams")
keywords <- c("venom", "snake", "snakebite", "antisnake")
myCorpus <- Corpus(VectorSource(snakebite$Text))
myCorpus <- tm_map(myCorpus, tolower)
myCorpus <- tm_map(myCorpus, removePunctuation)
myCorpus <- tm_map(myCorpus, removeNumbers)
myCorpus <- tm_map(myCorpus, removeWords, stopwords("english"))
myCorpus <- tm_map(myCorpus, removeWords, keywords)
myCorpus <- tm_map(myCorpus, content_transformer(function(x) gsub(x, pattern = "\\<nm\\>", replacement = "n8m")))
myCorpus <- tm_map(myCorpus, content_transformer(function(x) gsub(x, pattern = "[^[:alnum:][:blank:]?&/\\-]", replacement = "")))
myCorpus <- tm_map(myCorpus, content_transformer(function(x) gsub(x, pattern = "u00..", replacement = "")))
myCorpus <- iconv(myCorpus, from = "UTF-8", to = "ASCII", sub = "")  ## removing non asci charatcers
token_delim <- " \\t\\r\\n.!?,;\"()"
bitoken <- NGramTokenizer(myCorpus, Weka_control(min=2,max=2, delimiters = token_delim))
two_word <- data.frame(table(bitoken))
sort_two <- two_word[order(two_word$Freq,decreasing=TRUE),]
head(sort_two, 20)
color <- c("blue3", "blue4", "darkgoldenrod3", "darkgoldenrod4",
"chocolate1", "chocolate3", "chocolate1",
"green4","darkorchid3", "darkorchid4",
"firebrick1", "firebrick2", "firebrick3",
"purple3")
set.seed(4363)
wordcloud(sort_two$bitoken,sort_two$Freq,random.order=FALSE,min.freq = 20, scale=c(2,.9), colors = sample(color, 15, replace=TRUE ))
ggplot(head(sort_two,30), aes(reorder(bitoken,Freq), Freq)) +
geom_bar(stat = "identity") + coord_flip() +
xlab("Bigrams") + ylab("Frequency") +
ggtitle("Most frequent bigrams")
keywords <- c("venom", "snake", "snakebite", "antisnake")
myCorpus <- Corpus(VectorSource(snakebite$Text))
myCorpus <- tm_map(myCorpus, tolower)
myCorpus <- tm_map(myCorpus, removePunctuation)
myCorpus <- tm_map(myCorpus, removeNumbers)
myCorpus <- tm_map(myCorpus, removeWords, stopwords("english"))
myCorpus <- tm_map(myCorpus, removeWords, keywords)
myCorpus <- tm_map(myCorpus, content_transformer(function(x) gsub(x, pattern = "\\<nm\\>", replacement = "n8m")))
myCorpus <- tm_map(myCorpus, content_transformer(function(x) gsub(x, pattern = "[^[:alnum:][:blank:]?&/\\-]", replacement = "")))
myCorpus <- tm_map(myCorpus, content_transformer(function(x) gsub(x, pattern = "u00..", replacement = "")))
myCorpus <- iconv(myCorpus, from = "UTF-8", to = "ASCII", sub = "")  ## removing non asci charatcers
token_delim <- " \\t\\r\\n.!?,;\"()"
bitoken <- NGramTokenizer(myCorpus, Weka_control(min=2,max=2, delimiters = token_delim))
two_word <- data.frame(table(bitoken))
sort_two <- two_word[order(two_word$Freq,decreasing=TRUE),]
head(sort_two, 20)
color <- c("blue3", "blue4", "darkgoldenrod3", "darkgoldenrod4",
"chocolate1", "chocolate3", "chocolate1",
"green4","darkorchid3", "darkorchid4",
"firebrick1", "firebrick2", "firebrick3",
"purple3")
set.seed(4363)
wordcloud(sort_two$bitoken,sort_two$Freq,random.order=FALSE,min.freq = 20, scale=c(2.2,.9), colors = sample(color, 15, replace=TRUE ))
ggplot(head(sort_two,30), aes(reorder(bitoken,Freq), Freq)) +
geom_bar(stat = "identity") + coord_flip() +
xlab("Bigrams") + ylab("Frequency") +
ggtitle("Most frequent bigrams")
keywords <- c("venom", "snake", "snakebite", "antisnake")
myCorpus <- Corpus(VectorSource(snakebite$Text))
myCorpus <- tm_map(myCorpus, tolower)
myCorpus <- tm_map(myCorpus, removePunctuation)
myCorpus <- tm_map(myCorpus, removeNumbers)
myCorpus <- tm_map(myCorpus, removeWords, stopwords("english"))
myCorpus <- tm_map(myCorpus, removeWords, keywords)
myCorpus <- tm_map(myCorpus, content_transformer(function(x) gsub(x, pattern = "\\<nm\\>", replacement = "n8m")))
myCorpus <- tm_map(myCorpus, content_transformer(function(x) gsub(x, pattern = "[^[:alnum:][:blank:]?&/\\-]", replacement = "")))
myCorpus <- tm_map(myCorpus, content_transformer(function(x) gsub(x, pattern = "u00..", replacement = "")))
myCorpus <- iconv(myCorpus, from = "UTF-8", to = "ASCII", sub = "")  ## removing non asci charatcers
token_delim <- " \\t\\r\\n.!?,;\"()"
bitoken <- NGramTokenizer(myCorpus, Weka_control(min=2,max=2, delimiters = token_delim))
two_word <- data.frame(table(bitoken))
sort_two <- two_word[order(two_word$Freq,decreasing=TRUE),]
head(sort_two, 20)
color <- c("blue3", "blue4", "darkgoldenrod3", "darkgoldenrod4",
"chocolate1", "chocolate3", "chocolate1",
"green4","darkorchid3", "darkorchid4",
"firebrick1", "firebrick2", "firebrick3",
"purple3")
set.seed(4363)
wordcloud(sort_two$bitoken,sort_two$Freq,random.order=FALSE,min.freq = 20, scale=c(1.9,.9), colors = sample(color, 15, replace=TRUE ))
ggplot(head(sort_two,30), aes(reorder(bitoken,Freq), Freq)) +
geom_bar(stat = "identity") + coord_flip() +
xlab("Bigrams") + ylab("Frequency") +
ggtitle("Most frequent bigrams")
rm(list=ls())
library(rJava)
library(dplyr)
library(ggplot2)
library(tidyr)
library(reshape2)
library(wordcloud)
library(RWeka)
library(tm)
require(openNLP) #similar to RWeka, for NLP processing library(stringr)
library(text2vec)
library(LDAvis)
library(topicmodels)
library("wordcloud")
library("RColorBrewer")
library(SnowballC)
library(mallet)
snakebite<-read.csv('snakebite.csv', encoding = "UTF-8")
#head(snakebite, 20)
stop_words <- c('i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'could', 'take','yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself','they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these','those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do','does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while','of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'found','after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'co', 't','further', 'always', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each','few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than','too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've','y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn','needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn', 'jus', 'could', 'always', 'take', 'get', 'via', 'find', 'go', 'n', 'fcbeabcfcbedbabc' ,'fcbeabc', 'get', 'dont', 'say')
keywords <- c("venom", "snake", "snakebite", "antisnake")
#pre-processing
reviews <- gsub("'", "", snakebite$Text) # remove apostrophes
reviews = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", reviews)
reviews = gsub("@\\w+", "", reviews)
reviews <- gsub("[[:punct:]]", "", reviews) # replace punctuation with space
reviews <- gsub("[[:cntrl:]]", "", reviews) # replace control characters with space
reviews <- gsub("http\\w+","", reviews)
reviews <- gsub("^[[:space:]]+", "", reviews) # remove whitespace at beginning of documents
reviews <- gsub("[[:space:]]+$", "", reviews) # remove whitespace at end of documents
reviews <- iconv(reviews, "ASCII", "UTF-8", sub="byte")
corpus <- Corpus(VectorSource(reviews))
#start preprocessing
#Transform to lower case
corpus <-tm_map(corpus,content_transformer(tolower))
## remove special characaters
toSpace = content_transformer( function(x, pattern) gsub(pattern," ",x) )
corpus = tm_map(corpus, toSpace, "(f|ht)tp(s?)://(.*)[.][a-z]+")
corpus = tm_map( corpus, toSpace, "https*")
## removing punctuation
corpus <- tm_map(corpus, content_transformer(removePunctuation))
#Strip digits
corpus <- tm_map(corpus, removeNumbers)
## remove stopwords
corpus <- tm_map(corpus, content_transformer(removeWords), c(stopwords("english"), stop_words))
## exlude words used in keyword search
corpus = tm_map(corpus, removeWords, keywords)
## renaming certain words
corpus <- tm_map(corpus, content_transformer(function(x) gsub(x, pattern = "\\<nm\\>", replacement = "n8m")))
#remove whitespace
corpus <- tm_map(corpus, stripWhitespace)
#remove numbers
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, stemDocument)
writeLines(as.character(corpus[1:3])) # Check to see if it worked.
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
dtm <- DocumentTermMatrix(corpus, control = list(tokenize=BigramTokenizer, weighting = weightTf))
rowTotals <- apply(dtm , 1, sum) #Find the sum of words in each Document
dtm.new <- dtm[rowTotals> 0, ] #remove all docs without words
44
SEED = sample(1:1000000, 1) # Pick a random seed for replication
k <- 5
# NUMBER OF TOPICS
ldaOut <- LDA(dtm.new,k =k, method = "Gibbs", control = list(seed = SEED, burnin = 1000, thin =100,iter = 2000))
terms(ldaOut,10)
topicmodels2LDAvis <- function(x, ...){
post <- topicmodels::posterior(x)
if (ncol(post[["topics"]]) < 3) stop("The model must contain > 2 topics")
mat <- x@wordassignments
LDAvis::createJSON(
phi = post[["terms"]],
theta = post[["topics"]],
vocab = colnames(post[["terms"]]),
doc.length = slam::row_sums(mat, na.rm = TRUE), term.frequency = slam::col_sums(mat, na.rm = TRUE)
) }
topicmodels2LDAvis <- function(x, ...){
post <- topicmodels::posterior(x)
if (ncol(post[["topics"]]) < 3) stop("The model must contain > 2 topics")
mat <- x@wordassignments
LDAvis::createJSON(
phi = post[["terms"]],
theta = post[["topics"]],
vocab = colnames(post[["terms"]]),
doc.length = slam::row_sums(mat, na.rm = TRUE), term.frequency = slam::col_sums(mat, na.rm = TRUE)
) }
serVis(topicmodels2LDAvis(ldaOut, 5))
serVis(topicmodels2LDAvis(ldaOut, 5), out.dir = 'vis', open.browser = FALSE)
install.packages("servr")
rm(list=ls())
library(rJava)
library(dplyr)
library(ggplot2)
library(tidyr)
library(reshape2)
library(wordcloud)
library(RWeka)
library(tm)
require(openNLP) #similar to RWeka, for NLP processing library(stringr)
library(text2vec)
library(LDAvis)
library(topicmodels)
library("wordcloud")
library("RColorBrewer")
library(SnowballC)
library(mallet)
library()
snakebite<-read.csv('snakebite.csv', encoding = "UTF-8")
#head(snakebite, 20)
stop_words <- c('i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'could', 'take','yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself','they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these','those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do','does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while','of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'found','after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'co', 't','further', 'always', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each','few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than','too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've','y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn','needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn', 'jus', 'could', 'always', 'take', 'get', 'via', 'find', 'go', 'n', 'fcbeabcfcbedbabc' ,'fcbeabc', 'get', 'dont', 'say')
keywords <- c("venom", "snake", "snakebite", "antisnake")
#pre-processing
reviews <- gsub("'", "", snakebite$Text) # remove apostrophes
reviews = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", reviews)
reviews = gsub("@\\w+", "", reviews)
reviews <- gsub("[[:punct:]]", "", reviews) # replace punctuation with space
reviews <- gsub("[[:cntrl:]]", "", reviews) # replace control characters with space
reviews <- gsub("http\\w+","", reviews)
reviews <- gsub("^[[:space:]]+", "", reviews) # remove whitespace at beginning of documents
reviews <- gsub("[[:space:]]+$", "", reviews) # remove whitespace at end of documents
reviews <- iconv(reviews, "ASCII", "UTF-8", sub="byte")
corpus <- Corpus(VectorSource(reviews))
#start preprocessing
#Transform to lower case
corpus <-tm_map(corpus,content_transformer(tolower))
## remove special characaters
toSpace = content_transformer( function(x, pattern) gsub(pattern," ",x) )
corpus = tm_map(corpus, toSpace, "(f|ht)tp(s?)://(.*)[.][a-z]+")
corpus = tm_map( corpus, toSpace, "https*")
## removing punctuation
corpus <- tm_map(corpus, content_transformer(removePunctuation))
#Strip digits
corpus <- tm_map(corpus, removeNumbers)
## remove stopwords
corpus <- tm_map(corpus, content_transformer(removeWords), c(stopwords("english"), stop_words))
## exlude words used in keyword search
corpus = tm_map(corpus, removeWords, keywords)
## renaming certain words
corpus <- tm_map(corpus, content_transformer(function(x) gsub(x, pattern = "\\<nm\\>", replacement = "n8m")))
#remove whitespace
corpus <- tm_map(corpus, stripWhitespace)
#remove numbers
corpus <- tm_map(corpus, removeNumbers)
json_lda <- LDAvis::createJSON(phi = post[["terms"]], theta = post[["topics"]], vocab = colnames(post[["terms"]]), doc.length = slam::row_sums(mat, na.rm = TRUE), term.frequency = slam::col_sums(mat, na.rm = TRUE))
)
topicmodels2LDAvis <- function(x, ...){
post <- topicmodels::posterior(x)
if (ncol(post[["topics"]]) < 3) stop("The model must contain > 2 topics")
mat <- x@wordassignments
LDAvis::createJSON(
phi = post[["terms"]],
theta = post[["topics"]],
vocab = colnames(post[["terms"]]),
doc.length = slam::row_sums(mat, na.rm = TRUE), term.frequency = slam::col_sums(mat, na.rm = TRUE)
) }
serVis(topicmodels2LDAvis(ldaOut, 5), out.dir = 'vis', open.browser = FALSE))
serVis(topicmodels2LDAvis(ldaOut, 5), out.dir = 'vis', open.browser = FALSE)
corpus <- tm_map(corpus, stemDocument)
writeLines(as.character(corpus[1:3])) # Check to see if it worked.
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
dtm <- DocumentTermMatrix(corpus, control = list(tokenize=BigramTokenizer, weighting = weightTf))
rowTotals <- apply(dtm , 1, sum) #Find the sum of words in each Document
dtm.new <- dtm[rowTotals> 0, ] #remove all docs without words
44
SEED = sample(1:1000000, 1) # Pick a random seed for replication
k <- 5
# NUMBER OF TOPICS
ldaOut <- LDA(dtm.new,k =k, method = "Gibbs", control = list(seed = SEED, burnin = 1000, thin =100,iter = 2000))
terms(ldaOut,10)
serVis(topicmodels2LDAvis(ldaOut, 5), out.dir = 'vis', open.browser = FALSE)
library(servr)
