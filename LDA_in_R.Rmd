Before setting out to carry out the lda process on our document matrix, we have to find out the number of topics(K) in our data,since its an argument to the lda function.
```{r}


rm(list=ls()) 
library(rJava)
library(dplyr) 
library(ggplot2) 
library(tidyr) 
library(reshape2) 
library(wordcloud) 
library(RWeka)
library(tm)
require(openNLP) #similar to RWeka, for NLP processing library(stringr)
library(text2vec)
library(LDAvis)
library(topicmodels)
library("wordcloud")
library("RColorBrewer")
library(SnowballC)
library(mallet)

```

```{r}
snakebite<-read.csv('snakebite.csv', encoding = "UTF-8") 
#head(snakebite, 20)
```

```{r}
stop_words <- c('i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'could', 'take','yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself','they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these','those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do','does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while','of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'found','after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'co', 't','further', 'always', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each','few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than','too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've','y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn','needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn', 'jus', 'could', 'always', 'take', 'get', 'via', 'find', 'go', 'n', 'fcbeabcfcbedbabc' ,'fcbeabc', 'get', 'dont', 'say')

keywords <- c("venom", "snake", "snakebite", "antisnake")
#pre-processing
reviews <- gsub("'", "", snakebite$Text) # remove apostrophes 
reviews = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", reviews)

reviews = gsub("@\\w+", "", reviews)
reviews <- gsub("[[:punct:]]", "", reviews) # replace punctuation with space
reviews <- gsub("[[:cntrl:]]", "", reviews) # replace control characters with space
reviews <- gsub("http\\w+","", reviews)
reviews <- gsub("^[[:space:]]+", "", reviews) # remove whitespace at beginning of documents
reviews <- gsub("[[:space:]]+$", "", reviews) # remove whitespace at end of documents
reviews <- iconv(reviews, "ASCII", "UTF-8", sub="byte")
corpus <- Corpus(VectorSource(reviews))
#start preprocessing
#Transform to lower case
corpus <-tm_map(corpus,content_transformer(tolower))
## remove special characaters
toSpace = content_transformer( function(x, pattern) gsub(pattern," ",x) )
corpus = tm_map(corpus, toSpace, "(f|ht)tp(s?)://(.*)[.][a-z]+")
corpus = tm_map( corpus, toSpace, "https*")
## removing punctuation
corpus <- tm_map(corpus, content_transformer(removePunctuation))
#Strip digits
corpus <- tm_map(corpus, removeNumbers)
## remove stopwords
corpus <- tm_map(corpus, content_transformer(removeWords), c(stopwords("english"), stop_words))
## exlude words used in keyword search
corpus = tm_map(corpus, removeWords, keywords)
## renaming certain words
corpus <- tm_map(corpus, content_transformer(function(x) gsub(x, pattern = "\\<nm\\>", replacement = "n8m")))
#remove whitespace
corpus <- tm_map(corpus, stripWhitespace)
#remove numbers
corpus <- tm_map(corpus, removeNumbers)

```


Stemming text

```{r}

corpus <- tm_map(corpus, stemDocument) 
writeLines(as.character(corpus[1:3])) # Check to see if it worked.
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
dtm <- DocumentTermMatrix(corpus, control = list(tokenize=BigramTokenizer, weighting = weightTf))
rowTotals <- apply(dtm , 1, sum) #Find the sum of words in each Document
dtm.new <- dtm[rowTotals> 0, ] #remove all docs without words

```

Before setting out to carry out the lda process on our document matrix, we have to find out the number of topics(K) in our data,since its an argument to the lda function.


```{r}



44
SEED = sample(1:1000000, 1) # Pick a random seed for replication
k <- 5
# NUMBER OF TOPICS
ldaOut <- LDA(dtm.new,k =k, method = "Gibbs", control = list(seed = SEED, burnin = 1000, thin =100,iter = 2000))
terms(ldaOut,10)

```

Creating a function for visualising LDA using LDAvis package
' Convert the output of a topicmodels Latent Dirichlet Allocation to JSON

```{r}
topicmodels2LDAvis <- function(x, ...){
    post <- topicmodels::posterior(x)
    if (ncol(post[["topics"]]) < 3) stop("The model must contain > 2 topics")
    mat <- x@wordassignments 
    LDAvis::createJSON(
    phi = post[["terms"]],
    theta = post[["topics"]],
    vocab = colnames(post[["terms"]]),
    doc.length = slam::row_sums(mat, na.rm = TRUE), term.frequency = slam::col_sums(mat, na.rm = TRUE)
) }

```


Plotting the visualisation using the LDAvis package

```{r}

serVis(topicmodels2LDAvis(ldaOut, 5))
```


